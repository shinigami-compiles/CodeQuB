#!/usr/bin/env python3
"""
train_model.py (multi-task v2)

Train a multi-task ANN for:
"ANN-Based Code Quality & Bug Risk Prediction System Using Static & Process Metrics"

- Loads the synthetic dataset CSV generated by generate_dataset.py (v2).
- Splits into train/validation/test sets (70/15/15).
- Normalizes input features.
- Builds a shared-trunk ANN with task-specific heads:
    * Regression head: code_quality_score (0–100).
    * Classification head: bug_risk_class (6 classes).
- Uses a balanced multi-task loss:
    * 0.5 * MAE(code_quality_score) + 0.5 * SparseCategoricalCrossentropy(bug_risk_class)
- Trains with:
    * EarlyStopping on combined validation loss (val_loss).
    * ReduceLROnPlateau to adapt learning rate.

Evaluates on the test set:
    * Regression metrics: MAE, RMSE.
    * Classification metrics: Accuracy, confusion matrix, F1-score.

Outputs:
    - Trained model (.keras format).
    - Scaler for features.
    - Metadata (JSON) for UI & explainability:
        * feature_columns
        * feature_descriptions
        * feature_stats (mean, std)
        * num_bug_classes
        * label_mapping
"""

import os
import json
from typing import Tuple, Dict, Any

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    accuracy_score,
    confusion_matrix,
    f1_score,
    classification_report,
)
import joblib

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

RANDOM_SEED = 42

DATA_DIR = "data"
# Use the v2 dataset name here
DATA_CSV = os.path.join(DATA_DIR, "code_quality_bug_risk_dataset_15k_balanced_v2.csv")

MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, "multitask_ann_model.keras")
SCALER_PATH = os.path.join(MODEL_DIR, "feature_scaler.pkl")
METADATA_PATH = os.path.join(MODEL_DIR, "metadata.json")

TEST_SIZE = 0.15
VAL_SIZE = 0.15  # relative to total dataset
BATCH_SIZE = 64
EPOCHS = 120
LEARNING_RATE = 7e-4  # slightly lower for deeper network

# Balanced loss weights: both tasks equally important
QUALITY_LOSS_WEIGHT = 0.5
RISK_LOSS_WEIGHT = 0.5

NUM_BUG_CLASSES = 6


# ---------------------------------------------------------------------------
# Utility: Seeding
# ---------------------------------------------------------------------------

def set_global_seed(seed: int) -> None:
    """
    Set seeds for numpy and tensorflow for reproducibility.
    """
    np.random.seed(seed)
    tf.random.set_seed(seed)


# ---------------------------------------------------------------------------
# Data loading & preprocessing
# ---------------------------------------------------------------------------

def load_dataset(csv_path: str = DATA_CSV) -> pd.DataFrame:
    """
    Load the synthetic dataset from CSV.
    """
    if not os.path.exists(csv_path):
        raise FileNotFoundError(
            f"Dataset CSV not found at {csv_path}. "
            f"Please run generate_dataset.py first."
        )
    df = pd.read_csv(csv_path)
    return df


def get_feature_and_label_columns(df: pd.DataFrame) -> Tuple[list, list]:
    """
    Infer feature columns and label columns from the DataFrame.
    Assumes that label columns are:
        - code_quality_score
        - bug_risk_score
        - bug_risk_class
    All other columns are treated as features.
    """
    label_cols = ["code_quality_score", "bug_risk_score", "bug_risk_class"]
    for col in label_cols:
        if col not in df.columns:
            raise ValueError(f"Expected label column '{col}' not found in dataset.")

    feature_cols = [c for c in df.columns if c not in label_cols]
    return feature_cols, label_cols


def split_dataset(
    df: pd.DataFrame,
    feature_cols: list,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray,
           Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """
    Split the dataset into train/validation/test sets.

    Returns:
        X_train, X_val, X_test,
        y_train_dict, y_val_dict, y_test_dict
    """
    # Features
    X = df[feature_cols].values.astype(np.float32)

    # Labels (quality remains in 0–100 scale)
    y_quality = df["code_quality_score"].values.astype(np.float32)
    y_bug_class = df["bug_risk_class"].values.astype(int)

    # First, split off test set (15% of total)
    X_temp, X_test, yq_temp, yq_test, yc_temp, yc_test = train_test_split(
        X,
        y_quality,
        y_bug_class,
        test_size=TEST_SIZE,
        random_state=RANDOM_SEED,
        stratify=y_bug_class,
    )

    # Now split temp into train/val such that:
    # train = 70%, val = 15%, test = 15% of original
    # temp proportion = 85% (train + val). We want val to be (VAL_SIZE / (1 - TEST_SIZE)) of temp.
    val_ratio_in_temp = VAL_SIZE / (1.0 - TEST_SIZE)  # 0.15 / 0.85
    X_train, X_val, yq_train, yq_val, yc_train, yc_val = train_test_split(
        X_temp,
        yq_temp,
        yc_temp,
        test_size=val_ratio_in_temp,
        random_state=RANDOM_SEED,
        stratify=yc_temp,
    )

    # Wrap labels into dicts to align with model outputs
    y_train_dict = {
        "quality_output": yq_train,
        "risk_output": yc_train,
    }
    y_val_dict = {
        "quality_output": yq_val,
        "risk_output": yc_val,
    }
    y_test_dict = {
        "quality_output": yq_test,
        "risk_output": yc_test,
    }

    print("Dataset split sizes:")
    print(f"  Train: {X_train.shape[0]}")
    print(f"  Val:   {X_val.shape[0]}")
    print(f"  Test:  {X_test.shape[0]}")

    return X_train, X_val, X_test, y_train_dict, y_val_dict, y_test_dict


def normalize_features(
    X_train: np.ndarray,
    X_val: np.ndarray,
    X_test: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, StandardScaler]:
    """
    Fit a StandardScaler on train features and apply to train/val/test.
    """
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled, scaler


# ---------------------------------------------------------------------------
# Feature descriptions & stats (for UI / explainability)
# ---------------------------------------------------------------------------

def get_feature_descriptions() -> Dict[str, str]:
    """
    Human-readable descriptions for each feature.
    These will be stored in metadata.json and used by UI (tooltips, docs, reports).
    """
    return {
        "loc": "Lines of code in the file/module.",
        "num_functions": "Number of functions defined in the file.",
        "avg_function_length": "Average lines of code per function.",
        "cyclomatic_complexity": "Overall cyclomatic complexity of the file.",
        "max_nesting_depth": "Maximum depth of nested control structures.",
        "max_function_complexity": "Highest cyclomatic complexity of any single function.",
        "std_dev_function_complexity": "Variation (standard deviation) in function complexities.",
        "lint_warning_count": "Number of lint/style warnings reported for this file.",
        "comment_to_code_ratio": "Ratio of comment lines to code lines.",
        "review_comment_count": "Number of code review comments left on this file.",
        "test_coverage_percent": "Percentage of this file's code covered by automated tests.",
        "num_test_files_linked": "Number of test files linked to or covering this module.",
        "code_churn_recent": "Recent lines added/removed (measure of code churn).",
        "num_contributors": "Number of different contributors modifying this file.",
        "file_age_days": "Age of the file in days since creation.",
        "fan_in": "Number of other modules depending on this file.",
        "bug_count_total": "Total number of historical bugs linked to this file.",
        "bug_density": "Bug density (bugs per 1k lines of code).",
        "bugs_last_30_days": "Number of bugs reported in the last 30 days.",
        "bug_trend": "Recent bug trend (-1 decreasing, 0 stable, 1 increasing).",
        "test_fail_count_related": "Number of test failures recently related to this file.",
        "days_since_last_change": "Days since the file was last modified.",
    }


def compute_feature_stats(df: pd.DataFrame, feature_cols: list) -> Dict[str, Dict[str, float]]:
    """
    Compute simple statistics (mean, std) for each feature.
    These can be useful for UI context and explainability.

    Returns:
        {
            feature_name: {"mean": float, "std": float}
        }
    """
    stats: Dict[str, Dict[str, float]] = {}
    for col in feature_cols:
        series = df[col].astype(float)
        stats[col] = {
            "mean": float(series.mean()),
            "std": float(series.std(ddof=0)),  # population std (consistent with scaler)
        }
    return stats


# ---------------------------------------------------------------------------
# Model definition
# ---------------------------------------------------------------------------

def build_multitask_model(input_dim: int) -> tf.keras.Model:
    """
    Build the multi-task ANN model with:
        - Shared trunk
        - Task-specific regression head: code_quality_score
        - Task-specific classification head: bug_risk_class (6 classes)
    """
    inputs = layers.Input(shape=(input_dim,), name="features")

    # ----- Shared trunk -----
    x = layers.Dense(256, activation="relu", name="shared_dense_256")(inputs)
    x = layers.Dense(128, activation="relu", name="shared_dense_128")(x)
    x = layers.Dropout(0.3, name="shared_dropout_0_3")(x)
    x = layers.Dense(64, activation="relu", name="shared_dense_64")(x)

    # ----- Regression head (code quality) -----
    q = layers.Dense(32, activation="relu", name="quality_dense_32")(x)
    q = layers.Dense(16, activation="relu", name="quality_dense_16")(q)
    quality_output = layers.Dense(
        1, activation="linear", name="quality_output"
    )(q)

    # ----- Classification head (bug risk) -----
    r = layers.Dense(64, activation="relu", name="risk_dense_64")(x)
    r = layers.Dense(32, activation="relu", name="risk_dense_32")(r)
    risk_output = layers.Dense(
        NUM_BUG_CLASSES, activation="softmax", name="risk_output"
    )(r)

    model = models.Model(
        inputs=inputs,
        outputs={
            "quality_output": quality_output,
            "risk_output": risk_output,
        },
        name="multitask_code_quality_bug_risk_model_v2",
    )

    # ----- Multi-task loss & metrics -----
    losses = {
        "quality_output": tf.keras.losses.MeanAbsoluteError(),
        "risk_output": tf.keras.losses.SparseCategoricalCrossentropy(),
    }

    loss_weights = {
        "quality_output": QUALITY_LOSS_WEIGHT,  # 0.5
        "risk_output": RISK_LOSS_WEIGHT,        # 0.5
    }

    metrics = {
        "quality_output": [
            tf.keras.metrics.MeanAbsoluteError(name="mae"),
            tf.keras.metrics.MeanSquaredError(name="mse"),
        ],
        "risk_output": [
            tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
        ],
    }

    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)

    model.compile(
        optimizer=optimizer,
        loss=losses,
        loss_weights=loss_weights,
        metrics=metrics,
    )

    model.summary()
    return model


# ---------------------------------------------------------------------------
# Training
# ---------------------------------------------------------------------------

def train_model(
    model: tf.keras.Model,
    X_train: np.ndarray,
    y_train: Dict[str, np.ndarray],
    X_val: np.ndarray,
    y_val: Dict[str, np.ndarray],
) -> tf.keras.callbacks.History:
    """
    Train the model with:
        - EarlyStopping on combined validation loss (val_loss).
        - ReduceLROnPlateau to adapt learning rate when val_loss plateaus.
    """
    early_stopping = callbacks.EarlyStopping(
        monitor="val_loss",     # combined multi-task loss
        mode="min",
        patience=15,
        restore_best_weights=True,
        verbose=1,
    )

    lr_scheduler = callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=5,
        min_lr=1e-5,
        verbose=1,
    )

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1,
    )
    return history


# ---------------------------------------------------------------------------
# Evaluation
# ---------------------------------------------------------------------------

def evaluate_on_test_set(
    model: tf.keras.Model,
    X_test: np.ndarray,
    y_test: Dict[str, np.ndarray],
) -> None:
    """
    Evaluate the trained model on the test set and print metrics:
        - Regression: MAE, RMSE for code_quality_score.
        - Classification: Accuracy, confusion matrix, F1-score.
    """
    # Get predictions
    preds = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)
    # preds is a dict: {"quality_output": ..., "risk_output": ...}
    yq_true = y_test["quality_output"]
    yc_true = y_test["risk_output"]

    yq_pred = preds["quality_output"].reshape(-1)
    yc_pred_proba = preds["risk_output"]
    yc_pred = np.argmax(yc_pred_proba, axis=1)

    # Regression metrics (computed in original 0–100 scale)
    mae = mean_absolute_error(yq_true, yq_pred)
    mse = mean_squared_error(yq_true, yq_pred)
    rmse = np.sqrt(mse)

    print("\n--- Regression Metrics (Code Quality) ---")
    print(f"MAE : {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    # Classification metrics
    acc = accuracy_score(yc_true, yc_pred)
    f1_macro = f1_score(yc_true, yc_pred, average="macro")

    print("\n--- Classification Metrics (Bug Risk Class) ---")
    print(f"Accuracy : {acc:.4f}")
    print(f"F1-macro : {f1_macro:.4f}")
    print("\nConfusion Matrix:")
    print(confusion_matrix(yc_true, yc_pred))

    print("\nClassification Report:")
    print(classification_report(yc_true, yc_pred, digits=4))


# ---------------------------------------------------------------------------
# Saving model & metadata
# ---------------------------------------------------------------------------

def save_model_and_artifacts(
    model: tf.keras.Model,
    scaler: StandardScaler,
    feature_cols: list,
    feature_stats: Dict[str, Dict[str, float]],
    model_path: str = MODEL_PATH,
    scaler_path: str = SCALER_PATH,
    metadata_path: str = METADATA_PATH,
) -> None:
    """
    Save:
        - Keras model (.keras file)
        - Scaler (StandardScaler)
        - Metadata (feature names, descriptions, stats, bug class info)
    """
    os.makedirs(MODEL_DIR, exist_ok=True)

    # Save the model (.keras format)
    model.save(model_path)
    print(f"Model saved to: {model_path}")

    # Save the scaler
    joblib.dump(scaler, scaler_path)
    print(f"Scaler saved to: {scaler_path}")

    # Feature descriptions for UI
    feature_descriptions = get_feature_descriptions()

    # Save metadata
    metadata: Dict[str, Any] = {
        "feature_columns": feature_cols,
        "feature_descriptions": feature_descriptions,
        "feature_stats": feature_stats,
        "num_bug_classes": NUM_BUG_CLASSES,
        "label_mapping": {
            "0": "Very Low",
            "1": "Low",
            "2": "Slightly Risky",
            "3": "Medium",
            "4": "High",
            "5": "Critical",
        },
    }
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)
    print(f"Metadata saved to: {metadata_path}")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    set_global_seed(RANDOM_SEED)

    # 1. Load dataset
    df = load_dataset(DATA_CSV)
    feature_cols, label_cols = get_feature_and_label_columns(df)
    print("Feature columns:", feature_cols)
    print("Label columns:", label_cols)

    # 2. Compute feature stats (for metadata/UI)
    feature_stats = compute_feature_stats(df, feature_cols)

    # 3. Split dataset
    (
        X_train,
        X_val,
        X_test,
        y_train_dict,
        y_val_dict,
        y_test_dict,
    ) = split_dataset(df, feature_cols)

    # 4. Normalize features
    X_train_scaled, X_val_scaled, X_test_scaled, scaler = normalize_features(
        X_train, X_val, X_test
    )

    # 5. Build model
    model = build_multitask_model(input_dim=X_train_scaled.shape[1])

    # 6. Train model
    print("\nStarting training...")
    _ = train_model(
        model,
        X_train_scaled,
        y_train_dict,
        X_val_scaled,
        y_val_dict,
    )
    print("\nTraining complete.")

    # 7. Evaluate on test set
    print("\nEvaluating on test set...")
    evaluate_on_test_set(model, X_test_scaled, y_test_dict)

    # 8. Save model and artifacts (including rich metadata)
    save_model_and_artifacts(
        model=model,
        scaler=scaler,
        feature_cols=feature_cols,
        feature_stats=feature_stats,
        model_path=MODEL_PATH,
        scaler_path=SCALER_PATH,
        metadata_path=METADATA_PATH,
    )


if __name__ == "__main__":
    main()
